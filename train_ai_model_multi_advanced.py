import os
os.environ['OMP_NUM_THREADS'] = '12'
os.environ['MKL_NUM_THREADS'] = '12'
os.environ['OPENBLAS_NUM_THREADS'] = '12'

import pandas as pd
import numpy as np
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.ensemble import VotingClassifier
import joblib
import logging
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from scipy.stats.mstats import winsorize
from data.data_manager import get_candle_data
from feature_engineering_full_ultra_v2 import extract_features_full
from config import SYMBOLS

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ú©Ù†Ø³ÙˆÙ„
os.makedirs('notebooks', exist_ok=True)

# ØªÙ†Ø¸ÛŒÙ… ÙØ±Ù…Øª Ù„Ø§Ú¯
log_format = '%(asctime)s - %(levelname)s - %(message)s'
formatter = logging.Formatter(log_format)

# ØªÙ†Ø¸ÛŒÙ… FileHandler Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø¨Ø§ Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ UTF-8
file_handler = logging.FileHandler('notebooks/training_log.txt', encoding='utf-8')
file_handler.setFormatter(formatter)

# ØªÙ†Ø¸ÛŒÙ… StreamHandler Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù„Ø§Ú¯â€ŒÙ‡Ø§ Ø¯Ø± Ú©Ù†Ø³ÙˆÙ„
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)

# ØªÙ†Ø¸ÛŒÙ… logger
logger = logging.getLogger()
logger.setLevel(logging.INFO)
logger.handlers = []  # Ø­Ø°Ù handlerÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# ğŸ¯ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
TIMEFRAME = '4h'
WINDOW_SIZE = 100
LABEL_LOOKAHEAD = 12

def label_data(df: pd.DataFrame) -> pd.DataFrame:
    logging.info("Ø´Ø±ÙˆØ¹ Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
    df['future_return'] = df['close'].shift(-LABEL_LOOKAHEAD) / df['close'] - 1
    df['label'] = 'Hold'
    df.loc[df['future_return'] > 0.005, 'label'] = 'Buy'
    df.loc[df['future_return'] < -0.005, 'label'] = 'Sell'
    logging.info("Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
    return df.dropna()

def replace_outliers(df: pd.DataFrame, columns: list, threshold: float = 3) -> pd.DataFrame:
    logging.info("Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾Ø±Øª...")
    numeric_columns = [col for col in columns if df[col].dtype in [np.float64, np.int64]]
    for col in numeric_columns:
        if col in df.columns:
            df[col] = winsorize(df[col], limits=[0.05, 0.05])
            z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
            outliers_count = (z_scores > threshold).sum()
            if outliers_count > 0:
                logging.info(f"ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾Ø±Øª Ø¯Ø± {col}: {outliers_count}")
    logging.info("Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾Ø±Øª Ù…Ø¯ÛŒØ±ÛŒØª Ø´Ø¯Ù†Ø¯.")
    return df

def ensure_dtypes(df: pd.DataFrame) -> pd.DataFrame:
    logging.info("Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨...")
    for col in df.columns:
        if df[col].dtype == 'bool':
            df[col] = df[col].astype(bool)
        elif df[col].dtype == 'object':
            try:
                df[col] = pd.to_numeric(df[col], errors='raise')
            except:
                logging.warning(f"Ø³ØªÙˆÙ† {col} Ø¨Ù‡ Ø¹Ø¯Ø¯ ØªØ¨Ø¯ÛŒÙ„ Ù†Ø´Ø¯. Ø¨Ù‡ bool ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
                df[col] = df[col].astype(bool)
        else:
            df[col] = df[col].astype(float)
    logging.info("Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯.")
    return df

def plot_confusion_matrix(y_true, y_pred):
    logging.info("Ø±Ø³Ù… Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ...")
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig('notebooks/confusion_matrix.png')
    plt.close()
    logging.info("Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: notebooks/confusion_matrix.png")

def main():
    logging.info("Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...")
    all_data = []

    for symbol in SYMBOLS:
        logging.info(f"Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {symbol}")
        try:
            df = get_candle_data(symbol=symbol, limit=1000)
            if df.empty:
                logging.warning(f"Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ {symbol} Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                continue
            logging.info(f"ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {symbol}: {len(df)}")
            
            df['symbol'] = symbol
            logging.info(f"Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÛŒÚ†Ø±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ {symbol}...")
            df_feat = extract_features_full(df)
            logging.info(f"ÙÛŒÚ†Ø±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ {symbol} Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù†Ø¯: {df_feat.shape}")
            
            logging.info(f"Ø´Ø±ÙˆØ¹ Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ {symbol}...")
            df_labeled = label_data(df_feat)
            logging.info(f"Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ {symbol} Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯: {df_labeled.shape}")
            
            df_labeled = df_labeled.copy()
            df_labeled['symbol'] = symbol
            all_data.append(df_labeled)
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {symbol}: {e}")
            continue

    if not all_data:
        logging.error("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
        return

    logging.info("ØªØ±Ú©ÛŒØ¨ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ…Ø§Ù… Ù†Ù…Ø§Ø¯Ù‡Ø§...")
    df_final = pd.concat(all_data)
    df_final.dropna(inplace=True)
    logging.info(f"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù†Ø¯: {df_final.shape}")
    
    if 'news_sentiment' not in df_final.columns:
        df_final['news_sentiment'] = 0.0

    total = len(df_final)
    non_null = df_final['news_sentiment'].notna().sum()
    percentage = (non_null / total) * 100
    logging.info(f"Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ news_sentiment Ø¯Ø§Ø±Ù†Ø¯: {percentage:.2f}%  ({non_null}/{total})")

    fundamental_features = ['news_sentiment', 'fundamental_score', 'volume_score', 'news_score']
    technical_features = [col for col in df_final.columns if col not in fundamental_features + ['label', 'future_return', 'symbol']]

    df_final = replace_outliers(df_final, technical_features + fundamental_features)

    X = df_final.drop(columns=['label', 'future_return', 'symbol'])
    y = df_final['label']

    df_fundamental = df_final[df_final['news_sentiment'] != 0]
    X_fundamental = df_fundamental[fundamental_features]
    y_fundamental = df_fundamental['label']

    logging.info(f"ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„: {technical_features}")
    logging.info(f"ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„: {fundamental_features}")
    logging.info(f"Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ (X): {X.shape}")
    logging.info(f"Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ (X_fundamental): {X_fundamental.shape}")

    if X.empty:
        logging.error("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ (X) Ø®Ø§Ù„ÛŒ Ù‡Ø³ØªÙ†Ø¯. Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ú†Ø±Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø¹Ø¯ Ø§Ø² Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø­Ø°Ù Ø´Ø¯Ù†Ø¯.")
        return

    label_mapping = {'Sell': 0, 'Hold': 1, 'Buy': 2}
    y = y.map(label_mapping)
    y_fundamental = y_fundamental.map(label_mapping)

    logging.info("ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ...")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    logging.info(f"ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯: X_train={X_train.shape}, X_test={X_test.shape}")

    if not X_fundamental.empty:
        X_train_fund, X_test_fund, y_train_fund, y_test_fund = train_test_split(X_fundamental, y_fundamental, test_size=0.2, random_state=42)
        logging.info(f"ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯: X_train_fund={X_train_fund.shape}, X_test_fund={X_test_fund.shape}")
    else:
        logging.warning("Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")

    if X_train.columns.duplicated().any():
        logging.warning(f"Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ ØªÙˆ X_train: {X_train.columns[X_train.columns.duplicated()].tolist()}")
        X_train = X_train.loc[:, ~X_train.columns.duplicated()]
        X_test = X_test.loc[:, ~X_train.columns.duplicated()]

    logging.info("Ú†Ú© Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ (NaN/Inf) Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_train.fillna(X_train.mean(), inplace=True)
    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_test.fillna(X_test.mean(), inplace=True)
    if not X_fundamental.empty:
        X_train_fund.replace([np.inf, -np.inf], np.nan, inplace=True)
        X_train_fund.fillna(X_train_fund.mean(), inplace=True)
        X_test_fund.replace([np.inf, -np.inf], np.nan, inplace=True)
        X_test_fund.fillna(X_test.mean(), inplace=True)
    logging.info("Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù†Ø¯.")

    X_train = ensure_dtypes(X_train)
    X_test = ensure_dtypes(X_test)
    if not X_fundamental.empty:
        X_train_fund = ensure_dtypes(X_train_fund)
        X_test_fund = ensure_dtypes(X_test_fund)

    logging.info("Ø§Ø¹Ù…Ø§Ù„ SMOTE Ø¨Ø±Ø§ÛŒ Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§...")
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
    logging.info(f"Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ´Ø¯Ù‡: X_train_resampled={X_train_resampled.shape}")

    logging.info("Ø´Ø±ÙˆØ¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ CatBoost...")
    catboost = CatBoostClassifier(verbose=0, thread_count=12)
    catboost_params = {
        'iterations': [300, 500],
        'learning_rate': [0.05, 0.1],
        'depth': [4, 6],
        'l2_leaf_reg': [3, 5]
    }
    catboost_search = GridSearchCV(catboost, catboost_params, cv=3, scoring='f1_weighted', n_jobs=12)
    catboost_search.fit(X_train_resampled, y_train_resampled)
    logging.info("Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ CatBoost ØªÙ…ÙˆÙ… Ø´Ø¯.")
    logging.info(f"Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ CatBoost: {catboost_search.best_params_}")

    logging.info("Ø´Ø±ÙˆØ¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ LightGBM...")
    lgbm = LGBMClassifier(min_child_samples=10, min_split_gain=0.01, class_weight='balanced')
    lgbm_params = {
        'n_estimators': [300, 500],
        'learning_rate': [0.05, 0.1],
        'max_depth': [6, 8],
        'reg_lambda': [0, 1],
        'feature_fraction': [0.8, 0.9]
    }
    lgbm_search = GridSearchCV(lgbm, lgbm_params, cv=3, scoring='f1_weighted', n_jobs=12)
    lgbm_search.fit(X_train_resampled, y_train_resampled)
    logging.info("Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ LightGBM ØªÙ…ÙˆÙ… Ø´Ø¯.")
    logging.info(f"Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ LightGBM: {lgbm_search.best_params_}")

    if not X_fundamental.empty:
        logging.info("Ø´Ø±ÙˆØ¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ XGBoost (ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„)...")
        xgboost = XGBClassifier(eval_metric='mlogloss')
        xgboost_params = {
            'n_estimators': [200, 300],
            'learning_rate': [0.05, 0.1],
            'max_depth': [3, 5],
            'reg_lambda': [1, 3],
            'reg_alpha': [0, 1]
        }
        xgboost_search = GridSearchCV(xgboost, xgboost_params, cv=3, scoring='f1_weighted', n_jobs=12)
        xgboost_search.fit(X_train_fund, y_train_fund)
        logging.info("Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ XGBoost ØªÙ…ÙˆÙ… Ø´Ø¯.")
        logging.info(f"Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ XGBoost (ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„): {xgboost_search.best_params_}")
    else:
        logging.warning("XGBoost Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù†Ø¨ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ø§Ø¬Ø±Ø§ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

    logging.info("Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª ÙÛŒÚ†Ø±Ù‡Ø§...")
    best_catboost = catboost_search.best_estimator_
    importances = best_catboost.get_feature_importance()
    feature_names = X.columns
    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
    feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

    logging.info("ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… (CatBoost):")
    logging.info(feature_importance_df.head(10))

    total_importance = feature_importance_df['importance'].sum()
    threshold = 0.01 * total_importance
    selected_features = feature_importance_df[feature_importance_df['importance'] > threshold]['feature']
    logging.info(f"ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÚ†Ø±Ù‡Ø§ÛŒ Ù…Ù†ØªØ®Ø¨: {len(selected_features)}")

    X_train_selected = X_train_resampled[selected_features]
    X_test_selected = X_test[selected_features]
    logging.info(f"Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù†ØªØ®Ø¨: X_train_selected={X_train_selected.shape}, X_test_selected={X_test_selected.shape}")

    logging.info("ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡...")
    catboost_model = CatBoostClassifier(**catboost_search.best_params_, verbose=100, class_weights=[2, 1, 2], thread_count=12)
    lgbm_model = LGBMClassifier(**lgbm_search.best_params_, min_child_samples=10, min_split_gain=0.01, class_weight='balanced')
    if not X_fundamental.empty:
        xgboost_model = XGBClassifier(**xgboost_search.best_params_, eval_metric='mlogloss')

    logging.info("Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ØªØ±Ú©ÛŒØ¨ÛŒ...")
    if not X_fundamental.empty:
        ensemble_model = VotingClassifier(estimators=[
            ('catboost', catboost_model),
            ('lightgbm', lgbm_model),
            ('xgboost_fundamental', xgboost_model)
        ], voting='soft', weights=[0.4, 0.4, 0.2])
    else:
        ensemble_model = VotingClassifier(estimators=[
            ('catboost', catboost_model),
            ('lightgbm', lgbm_model)
        ], voting='soft', weights=[0.5, 0.5])

    ensemble_model.fit(X_train_selected, y_train_resampled)
    logging.info("Ù…Ø¯Ù„ ØªØ±Ú©ÛŒØ¨ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯.")
    
    y_pred_ensemble = ensemble_model.predict(X_test_selected)
    logging.info("Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ ØªØ±Ú©ÛŒØ¨ÛŒ:")
    logging.info(classification_report(y_test, y_pred_ensemble))

    plot_confusion_matrix(y_test, y_pred_ensemble)

    joblib.dump(ensemble_model, 'models/ensemble_model_multi_with_fundamental.pkl')
    logging.info("Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: ensemble_model_multi_with_fundamental.pkl")

if __name__ == "__main__":
    try:
        main()
        logging.info("Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")
    finally:
        input("Press Enter to exit...")