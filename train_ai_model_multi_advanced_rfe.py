import pandas as pd
import numpy as np
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_selection import RFECV
from sklearn.ensemble,çœ‹çœ‹çœ‹ VotingClassifier
import joblib
from data.data_manager import get_candle_data
from feature_engineering_full_ultra_v2 import extract_features_full
from config import SYMBOLS

# ðŸŽ¯ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
TIMEFRAME = '4h'
WINDOW_SIZE = 100
LABEL_LOOKAHEAD = 12

def label_data(df: pd.DataFrame) -> pd.DataFrame:
    df['future_return'] = df['close'].shift(-LABEL_LOOKAHEAD) / df['close'] - 1
    df['label'] = 'Hold'
    df.loc[df['future_return'] > 0.005, 'label'] = 'Buy'
    df.loc[df['future_return'] < -0.005, 'label'] = 'Sell'
    return df.dropna()

def main():
    all_data = []

    for symbol in SYMBOLS:
        print(f"ðŸ“Š Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ {symbol}")
        try:
            df = get_candle_data(symbol=symbol, limit=1000)
            if df.empty:
                continue
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†Ø¯Ù„ Ø¨Ø±Ø§ÛŒ {symbol}: {e}")
            continue

        df['symbol'] = symbol
        df_feat = extract_features_full(df)
        df_labeled = label_data(df_feat)
        df_labeled = df_labeled.copy()
        df_labeled['symbol'] = symbol
        all_data.append(df_labeled)

    if not all_data:
        print("â›” Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
        return

    df_final = pd.concat(all_data)
    df_final.dropna(inplace=True)
    if 'news_sentiment' not in df_final.columns:
        df_final['news_sentiment'] = 0.0

    total = len(df_final)
    non_null = df_final['news_sentiment'].notna().sum()
    percentage = (non_null / total) * 100
    print(f"ðŸ“Š Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ news_sentiment Ø¯Ø§Ø±Ù†Ø¯: {percentage:.2f}%  ({non_null}/{total})")

    # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„
    fundamental_features = ['news_sentiment', 'fundamental_score', 'volume_score', 'news_score']
    # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ (Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù…Ù†Ù‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„)
    technical_features = [col for col in df_final.columns if col not in fundamental_features + ['label', 'future_return', 'symbol']]

    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ CatBoost Ùˆ LightGBM
    X = df_final.drop(columns=['label', 'future_return', 'symbol'])
    y = df_final['label']

    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ Ø¨Ø±Ø§ÛŒ XGBoost
    df_fundamental = df_final[df_final['news_sentiment'] != 0]  # ÙÙ‚Ø· Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ news_sentiment Ø¯Ø§Ø±Ù†
    X_fundamental = df_fundamental[fundamental_features]
    y_fundamental = df_fundamental['label']

    print("ðŸ” ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„:", technical_features)
    print("ðŸ” ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„:", fundamental_features)

    # ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¹Ø¯Ø¯
    label_mapping = {'Sell': 0, 'Hold': 1, 'Buy': 2}
    y = y.map(label_mapping)
    y_fundamental = y_fundamental.map(label_mapping)

    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ CatBoost Ùˆ LightGBM
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ XGBoost
    if not X_fundamental.empty:
        X_train_fund, X_test_fund, y_train_fund, y_test_fund = train_test_split(X_fundamental, y_fundamental, test_size=0.2, random_state=42)
    else:
        print("âš ï¸ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        return

    # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ RFE (CatBoost)
    model_for_rfe = CatBoostClassifier(verbose=0)
    rfe_params = {
        'iterations': [50, 100],
        'learning_rate': [0.05, 0.1],
        'depth': [4, 6]
    }
    rfe_search = GridSearchCV(model_for_rfe, rfe_params, cv=3, scoring='f1_weighted', n_jobs=-1)
    rfe_search.fit(X_train, y_train)
    best_rfe_model = rfe_search.best_estimator_
    print("ðŸ“Š Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„ RFE:", rfe_search.best_params_)

    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² RFECV Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    rfecv = RFECV(estimator=best_rfe_model, step=1, cv=3, scoring='f1_weighted')
    rfecv.fit(X_train, y_train)
    selected_features_rfe = X_train.columns[rfecv.support_]
    print("ðŸ“Š ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù†ØªØ®Ø¨ RFE Ø¨Ø±Ø§ÛŒ CatBoost Ùˆ LightGBM:", selected_features_rfe.tolist())

    # Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ XGBoost
    rfecv_fund = RFECV(estimator=best_rfe_model, step=1, cv=3, scoring='f1_weighted')
    rfecv_fund.fit(X_train_fund, y_train_fund)
    selected_fund_features_rfe = X_train_fund.columns[rfecv_fund.support_]
    print("ðŸ“Š ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù†ØªØ®Ø¨ RFE Ø¨Ø±Ø§ÛŒ XGBoost (ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„):", selected_fund_features_rfe.tolist())

    # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ CatBoost
    catboost = CatBoostClassifier(verbose=0, class_weights=[2, 1, 2])
    catboost_params = {
        'iterations': [300, 500, 1000],
        'learning_rate': [0.01, 0.05, 0.1],
        'depth': [4, 6, 8],
        'l2_leaf_reg': [1, 3, 5]
    }
    catboost_search = GridSearchCV(catboost, catboost_params, cv=3, scoring='f1_weighted', n_jobs=-1)
    catboost_search.fit(X_train[selected_features_rfe], y_train)
    print("ðŸ“Š Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ CatBoost:", catboost_search.best_params_)

    # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ LightGBM
    lgbm = LGBMClassifier()
    lgbm_params = {
        'n_estimators': [300, 500, 1000],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [4, 6, 8],
        'reg_lambda': [0, 1, 5]
    }
    lgbm_search = GridSearchCV(lgbm, lgbm_params, cv=3, scoring='f1_weighted', n_jobs=-1)
    lgbm_search.fit(X_train[selected_features_rfe], y_train)
    print("ðŸ“Š Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ LightGBM:", lgbm_search.best_params_)

    # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ XGBoost Ø¨Ø±Ø§ÛŒ ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„
    xgboost = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
    xgboost_params = {
        'n_estimators': [200, 300, 500],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 5, 7],
        'reg_lambda': [1, 3, 5],
        'reg_alpha': [0, 1, 3]
    }
    xgboost_search = GridSearchCV(xgboost, xgboost_params, cv=3, scoring='f1_weighted', n_jobs=-1)
    xgboost_search.fit(X_train_fund[selected_fund_features_rfe], y_train_fund)
    print("ðŸ“Š Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ XGBoost (ÙØ§Ù†Ø¯Ø§Ù…Ù†ØªØ§Ù„):", xgboost_search.best_params_)

    # ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡
    catboost_model = CatBoostClassifier(**catboost_search.best_params_, verbose=100, class_weights=[2, 1, 2])
    lgbm_model = LGBMClassifier(**lgbm_search.best_params_)
    xgboost_model = XGBClassifier(**xgboost_search.best_params_, use_label_encoder=False, eval_metric='mlogloss')

    # ØªØ±Ú©ÛŒØ¨ Ù…Ø¯Ù„â€ŒÙ‡Ø§ (Ensemble)
    ensemble_model = VotingClassifier(estimators=[
        ('catboost', catboost_model),
        ('lightgbm', lgbm_model),
        ('xgboost_fundamental', xgboost_model)
    ], voting='soft', weights=[0.4, 0.4, 0.2])

    # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ØªØ±Ú©ÛŒØ¨ÛŒ
    ensemble_model.fit(X_train[selected_features_rfe], y_train)
    y_pred_ensemble = ensemble_model.predict(X_test[selected_features_rfe])
    print("ðŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ ØªØ±Ú©ÛŒØ¨ÛŒ:")
    print(classification_report(y_test, y_pred_ensemble))

    # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
    joblib.dump(ensemble_model, 'models/ensemble_model_multi_rfe_with_fundamental.pkl')
    print("âœ… Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: ensemble_model_multi_rfe_with_fundamental.pkl")

if __name__ == "__main__":
    main()